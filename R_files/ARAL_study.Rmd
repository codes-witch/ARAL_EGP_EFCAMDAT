---
title: "NLP-powered Quantitative Verification of the English Grammar Profileâ€™s Structure- Level Assignment"
output:
  pdf_document: default
  html_notebook: default
---


## Preparing the environment

```{r}
setwd("/home/XXXXX/Documents/ARAL_EGP_EFCAMDAT/R_files/")
source("/home/XXXXX/Documents/ARAL_EGP_EFCAMDAT/R_files/functions.R")
library(ggplot2)
library(gridExtra)
library("dplyr")
library("stringr")
library(tidyr)
library("irr")
library(tools)
library(knitr)
library("tidytext")
library("tokenizers")
load("/home/XXXXX/Documents/ARAL_EGP_EFCAMDAT/R_files/ef2_20181231.RData") # This file
egp_list <- read.csv("/home/daniela/Documents/ARAL_EGP_EFCAMDAT/egp_list.csv", sep = ";")
all_features <- readLines("all_features.txt")
opts_chunk$set(cache = TRUE)
```

NOTE: we do not provide access to the corpus, but we provide the files with the <System> annotations. The following code chunks are simply to illustrate the selection and sampling of the learners. For reproduction, start at the point where the texts have been annotated with the structures.

First, we add a column with the CEFR level that each text should belong to based on the EFCAMDAT level

```{r}
# rename the "level" column (which is at index 2)
colnames(ef2)[2] <- "ef_level"
# remove unnecessary colums
ef2 <- ef2[, !colnames(ef2) %in% c("text", "corrected", "nationality")]
# add cefr level
ef2_prepared <- add_cefr_from_ef_levels(ef2)

# mark those rows with the first and last units of a CEFR level
# This will help us group them
ef2_prepared <- add_combined_level(ef2_prepared)
```

Now, get the learnerIDs of people for whome we have all necessary units. Generally, these are the first three and last three of their respective CEFR level. 
In the case of A1, They need to have the first six units and the last three units. In the case of C2, they need only the first three units.

```{r}
# Get those students that completed each level
# (except C2, for which we only need the students who completed the first three units)


cefr_ef_lvls <- list( # These are the EFCamDat levels that correspond to each CEFR level
  "A1" = c(1:3),
  "A2" = c(4:6),
  "B1" = c(7:9),
  "B2" = c(10:12),
  "C1" = c(13:15),
  "C2" = c(16)
)

# A named list (dictionary) mapping levels to the students that finished them
list_students_finsihed <- list()


for (cur_cefr_lvl in names(cefr_ef_lvls)) {
  # Minimum number of texts that the learner should have completed within the subset that will be taken into account
  min_n_texts <- 6 # (only top 3 and highest and lowest units in the level)

  if (cur_cefr_lvl == "A1") {
    min_n_texts <- 9 # top 6 lowest + top 3 highest
  } else if (cur_cefr_lvl == "C2") {
    min_n_texts <- 3 # top 3 lowest
  }

  learners_finished <- ef2_prepared %>%
    filter(cefr_level == cur_cefr_lvl & !is.na(combined_cefr)) %>% # current cefr level top high/low units
    group_by(learnerID) %>%
    summarise(units_count = n_distinct(ef_level, unit)) %>% # number of unique ef_level-unit pairs each learner has completed
    filter(units_count == min_n_texts) %>% # only those that have completed all units at least once
    pull(learnerID)

  list_students_finsihed[[cur_cefr_lvl]] <- learners_finished
}

# Check how many students we have for each level:
for (lvl in names(list_students_finsihed)) {
  print(paste(lvl, length(list_students_finsihed[[lvl]])))
}

# 49 is the minimum number of students having finished a level (C1)
min_num_students <- min(sapply(list_students_finsihed, length))

print(min_num_students)
```

We now know that in order to have a balanced sample, we can have at most 49 students of any given level in the combined CEFR levels.
Namely, each combined class will have 49 students from the lower level and 49 students from the higher level.

From the learners who completed the levels, we must sample 49 without replacement.
These people's texts in the first and last three units of their CEFR levels will be the ones to be analyzed.
In total, at each combined level we will have scripts written by 98 students and we will use them as a "snapshot" of their proficiency once the lower level reached.

Now, we must sample the students and extract their texts to be analyzed by the <System> tool.

```{r}
sampled_learners <- list()
set.seed(1234)
for (level in names(cefr_ef_lvls)) {
  sampled_learners[[level]] <- sample(list_students_finsihed[[level]], min_num_students, replace = FALSE)
}

# To check that the sampling is deterministic, run this line several times and check the learnerIDs
# sapply(sampled_learners, function(x) sort(unique(x)))
```

Students have been sampled. Now, we need to extract their texts and put them in .txt files to be analyzed by the python script.

```{r}
input_path <- "/home/daniela/Documents/ARAL_EGP_EFCAMDAT/text_annotation/data/input_21Nov24/"
df_filtered_all <- ef2_prepared[0, ]

# make dataframe with the learners that we want at each level
for (lvl in names(cefr_ef_lvls)) {
  filtered_df <- ef2_prepared %>%
    filter(cefr_level == lvl & learnerID %in% sampled_learners[[lvl]] & !is.na(combined_cefr))

  df_filtered_all <- rbind(df_filtered_all, filtered_df)
}

# Put the texts into .txt files
# ef_text_2_txt(df_filtered_all, input_path)
```

Now, we run the Python script and wait. It will take a while. 

Once the scripts have been annotated, we need to count how many times each student at the individual CEFR levels have used each structure.

NOTE: here, we read in the annotations. From now on, the code can be run to reproduce our results.
```{r}
# For getting the output path dynamically given the level
# file_format can be txt or csv
get_output_path <- function(combined_lvl, file_format = "csv") {
  # NOTE change the output path if needed
  return(paste0("../text_annotation/data/output_21Nov24/", combined_lvl, "/", file_format, "/"))
}

get_learner_ids_by_lvl <- function(combined_lvl) {
  last_lvl <- str_split_i(combined_lvl, "_", 2) # high level in the combination
  first_lvl <- str_split_i(combined_lvl, "_", 1)
  if (last_lvl == "A1") { # If it's A0_A1, all learners are from A1
    learner_ids <- sampled_learners[["A1"]]
  } else { # get learners from the last level and the first one
    learner_ids <- sort(unique(append(sampled_learners[[first_lvl]], sampled_learners[[last_lvl]])))
  }
}

# Calls the get_feat_count_per_student with the necessary parameters given the level combination
count_constrs_by_lvl <- function(combined_lvl, make_long = TRUE) {
  learner_ids <- get_learner_ids_by_lvl(combined_lvl)

  return(get_feat_count_per_student(
    directory_path = get_output_path(combined_lvl),
    learner_ids = learner_ids,
    make_long = make_long
  ))
}

# These dfs contain the feature counts for all learners at each combined level
a0a1_feat_counts <- count_constrs_by_lvl("A0_A1", make_long = FALSE)
a1a2_feat_counts <- count_constrs_by_lvl("A1_A2", make_long = FALSE)
a2b1_feat_counts <- count_constrs_by_lvl("A2_B1", make_long = FALSE)
b1b2_feat_counts <- count_constrs_by_lvl("B1_B2", make_long = FALSE)
b2c1_feat_counts <- count_constrs_by_lvl("B2_C1", make_long = FALSE)
c1c2_feat_counts <- count_constrs_by_lvl("C1_C2", make_long = FALSE)
```

Now it is necessary to normalize these counts by number of words written by each individual student in the respective (combined) levels and normalize it by 100 words

```{r}
a0a1_word_counts <- get_learner_word_count_df(get_learner_ids_by_lvl("A0_A1"), get_output_path("A0_A1", "text"))
a1a2_word_counts <- get_learner_word_count_df(get_learner_ids_by_lvl("A1_A2"), get_output_path("A1_A2", "text"))
a2b1_word_counts <- get_learner_word_count_df(get_learner_ids_by_lvl("A2_B1"), get_output_path("A2_B1", "text"))
b1b2_word_counts <- get_learner_word_count_df(get_learner_ids_by_lvl("B1_B2"), get_output_path("B1_B2", "text"))
b2c1_word_counts <- get_learner_word_count_df(get_learner_ids_by_lvl("B2_C1"), get_output_path("B2_C1", "text"))
c1c2_word_counts <- get_learner_word_count_df(get_learner_ids_by_lvl("C1_C2"), get_output_path("C1_C2", "text"))


a0a1_normalized <- cbind(get_feats_normalized_students(a0a1_feat_counts, a0a1_word_counts), text_level = "A0_A1")
a1a2_normalized <- cbind(get_feats_normalized_students(a1a2_feat_counts, a1a2_word_counts), text_level = "A1_A2")
a2b1_normalized <- cbind(get_feats_normalized_students(a2b1_feat_counts, a2b1_word_counts), text_level = "A2_B1")
b1b2_normalized <- cbind(get_feats_normalized_students(b1b2_feat_counts, b1b2_word_counts), text_level = "B1_B2")
b2c1_normalized <- cbind(get_feats_normalized_students(b2c1_feat_counts, b2c1_word_counts), text_level = "B2_C1")
c1c2_normalized <- cbind(get_feats_normalized_students(c1c2_feat_counts, c1c2_word_counts), text_level = "C1_C2")
```

Now we know the normalized frequency of each costruct for the students at each level combination. 


First, we get rid of all structures which were not found in any student's writings:

```{r}
# put all dataframes together
normalized_all_levels <- rbind(a0a1_normalized, a1a2_normalized, a2b1_normalized, b1b2_normalized, b2c1_normalized, c1c2_normalized)

# if there are any NAs, it's because texts by some students could not be processed. Drop these rows
normalized_all_levels <- normalized_all_levels %>%
  drop_na()

zero_constructs <- normalized_all_levels %>%
  group_by(feature) %>%
  filter(all(total == 0)) %>%
  pull(feature) %>%
  unique()

# remove these zero-structures from our data
normalized_all_levels <- normalized_all_levels %>%
  filter(!feature %in% zero_constructs)
```

We can start statistically analyzing the structure frequency distributions per learner. 

```{r}
# check for normality

library("ggpubr")
normalized_all_levels %>%
  pull(total) %>%
  ggdensity()
```

We observe that the data is not normally distrubuted, with a vast majority of the constructs having close to zero frequecy. 
Log transforming the data does not reveal a normal distribution either.


```{r}
# Log-transform and visualize
library("ggpubr")
normalized_all_levels %>%
  pull(total) %>%
  log() %>%
  ggdensity()
```

It is therefore wise to use non-parametric tests.

First, we will apply the Kruskal-Wallis test to identify any structures that do not vary significantly across proficiency levels.


```{r}
constructs <- unique(normalized_all_levels$feature) # These are all non-zero structures
kw_results <- list()  # mapping between structures and their Kruskal-Wallis results
kw_non_sign_constructs <- c()
kw_sign_constructs <- c()
kw_nan_results_constructs <- c()

for (constr in constructs) {
  # df with only the current structure. Check whether there are no occurrences of the feature.
  feature_dataframe <- normalized_all_levels %>%
    filter(feature == constr)

  # Fill in kw result list
  kw_results[[constr]] <- normalized_all_levels %>%
    filter(feature == constr) %>%
    kruskal.test(total ~ text_level, data = .)

  p_value <- kw_results[[constr]]$p.value

  if (is.nan(p_value)) { # sanity check: kw_nan_results_structures will remain empty because we are discarding all features with no occurrences
    kw_nan_results_constructs <- append(kw_nan_results_constructs, constr)
  } else if (p_value > 0.05) {
    kw_non_sign_constructs <- append(kw_non_sign_constructs, constr)
  } else {
    kw_sign_constructs <- append(kw_sign_constructs, constr)
  }
}
```

A total of 155 structures do not show significant differences across levels.
```{r}
length(kw_non_sign_constructs)

# From the EGP list file, get the structures with where no significant differences were found:
uninformative_kw <- egp_list %>%
  filter(EGP_ID %in% kw_non_sign_constructs)

write.table(uninformative_kw, file = "kw_discarded_structures.csv", sep = ";") # Create the CSV
```


```{r}
# For all significant structure, do Wilcoxon Rank Sum tests to check for significant differences between the current level and the previous one. 
levels <- c("A0_A1", "A1_A2", "A2_B1", "B1_B2", "B2_C1", "C1_C2")

# store p-values from wilcoxon tests for each structure
wcx_pvals <- data.frame(matrix(ncol = 6, nrow = length(kw_sign_constructs)))
names(wcx_pvals) <- c("feature", "A1", "A2", "B1", "B2", "C1")

# For all relevant structures, do a wilcoxon-test between all consecutive levels
for (f in 1:length(kw_sign_constructs)) {
  constr <- kw_sign_constructs[f]
  # build the rows for the p-values column by column
  wcx_pvals_vec <- c()

  for (i in 1:(length(levels) - 1)){
    level1 <- levels[i]
    level2 <- levels[i + 1]
    print(paste0("Structure: ", constr))

    appears_in_lev1 <- sum(filter(normalized_all_levels, text_level == level1, feature == constr)$total) > 0
    appears_in_lev2 <- sum(filter(normalized_all_levels, text_level == level2, feature == constr)$total) > 0

    # Check that the current feature appears in the level before and in the current level
    if (!appears_in_lev1 || !appears_in_lev2) {
      if (!appears_in_lev1 && !appears_in_lev2) {
        print(paste0("Contsruct ", constr, " does not appear in any texts written by ", level1, " or ", level2, " students"))
      } else if (!appears_in_lev1) {
        print(paste0("Contsruct ", constr, " does not appear in any texts written by ", level1, " students"))
      } else if (!appears_in_lev2) {
        print(paste0("Contsruct ", constr, " does not appear in any texts written by ", level2, " students"))
      }

      wcx_pvals_vec <- append(wcx_pvals_vec, NA)

    } else { # It must appear in BOTH levels to be compared
      wcx_result <- normalized_all_levels %>%
        filter(feature == constr, text_level %in% c(level1, level2)) %>%
        wilcox.test(total ~ text_level, data = ., alternative = "less")

      if (wcx_result$p.value <= 0.05) {
        wcx_pvals_vec <- append(wcx_pvals_vec, wcx_result$p.value)
      } else {
        wcx_pvals_vec <- append(wcx_pvals_vec, NA)
      }
    }
  }

  constr_pval_row <- append(as.character(constr), wcx_pvals_vec)

  wcx_pvals[f, ] <- constr_pval_row

}
```


Based on the p-values, we predict the level of acquisition operationalized as the first level in which the per-student frequencies differ significantly between the start and end of the level.

```{r}
wcx_level_predict <-  data.frame(feature = kw_sign_constructs) %>%
  add_feature_level() %>%
  rename(EGP_level = feat_level) %>%
  rename(construct = feature) %>%
  mutate(first_sign_p_pred = NA, signif_at_EGP_level = NA)
# Find first significant diff
wcx_level_predict <- fill_first_signif_pred(wcx_pvals, wcx_level_predict)

# fill in TRUE FALSE, is there a significant difference at the level the EGP says?
wcx_level_predict <- fill_is_sig_at_lvl(wcx_pvals, wcx_level_predict) %>%
  filter(EGP_level != "C2")
```


```{r}
View(wcx_level_predict)
```

Now that we have some frequency-based predictions for the alignment between structures and levels, we would like to see how they compare with the EGP alignments.

For this, we use precision, recall, and F1-scores. 

```{r}
# Calculate precision, recall and F1 for t-tests predictions

# get only the predictions for levels
# we lack enough C2 data to look at both beginning and end, so our approach will never assign C2 as an acquisition level. Therefore, we filter out EGP-C2 structures
# Also, if there are no significant differences between consecutive combined levels for the structure, we cannot make a prediction and get an NA. Ignore those.
wcx_predictions <- wcx_level_predict %>%
  filter(EGP_level != "C2") %>%
  filter(!is.na(first_sign_p_pred)) %>%
  pull(first_sign_p_pred)

# get the EGP level assignments
egp_expected <- wcx_level_predict %>%
  filter(EGP_level != "C2" & !is.na(first_sign_p_pred)) %>%
  pull(EGP_level)


# This calculates the precision and recall for having predicted the exact level.
exact_metrics <- get_exact_precision_recall_f1(egp_expected, wcx_predictions)

# This calculates the precision and recall for having predicted the exact level, or one level above or below.
neighbors_metrics <- get_neigh_prec_rec_f1(egp_expected, wcx_predictions)


print(exact_metrics)

print(neighbors_metrics)
```


Visualize how predictions relate to the EGP levels
```{r}
library(caret)


cm <- confusionMatrix(data = as.factor(wcx_predictions), reference = as.factor(egp_expected))
cm
conf_matrix <- table("EGP" = egp_expected, "Predicted" = wcx_predictions)
cf <- as.data.frame(conf_matrix)


conf_matrix_heatmap <-  ggplot(data = as.data.frame(cf), aes(x = Predicted, y =  EGP, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "black") +
  theme_light() +
  guides(scale = NULL) +
  coord_fixed() +
  scale_fill_gradient(low = "white", high = "#3c8b3c")

ggsave("plots/heatmap.png", plot = conf_matrix_heatmap)


# caculate weighted Cohen's Kappa
df_for_kappa <- wcx_level_predict %>%
  filter(!is.na(first_sign_p_pred)) %>%
  select(EGP_level, first_sign_p_pred)

df_for_kappa$EGP_level <- as.factor(df_for_kappa$EGP_level)
df_for_kappa$first_sign_p_pred <- as.factor(df_for_kappa$first_sign_p_pred)

levels(df_for_kappa$first_sign_p_pred)

kappa2(df_for_kappa, "squared")
```


```{r}
# These are the EGP-assigned levels of all structures:
constr_lvl_df <- data.frame(feature = all_features)
constr_lvl_df <- add_feature_level(constr_lvl_df)

# See how many structures were zero that did NOT belong to C2:
constr_lvl_df %>%
  filter(feat_level != "C2" & feature %in% zero_constructs) %>%
  count()
```

Here, we create the line plots to observe the development of the frequencies across levels.  

```{r}

# # Save boxplots in a file
# for (construct in constructs) {
#   bp <- normalized_all_levels %>%
#     get_boxplot_construct(construct)

#   ggsave(paste0("plots/", constr_lvl_df[constr_lvl_df$feature == construct, "feat_level"], "_", construct, "_boxplot.png"), width= 5.40, height = 4.20, plot = bp)

#   lp <- normalized_all_levels %>%
#     get_lineplot(construct)

#   ggsave(paste0("plots/", constr_lvl_df[constr_lvl_df$feature == construct, "feat_level"], "_", construct, "_lineplot.png"), width= 5.40, height = 4.20, plot = lp)
# }
```

We have 120 structures that do not appear in any of the texts. Observe the percetages of missing structures per level

```{r}
get_percent_constr_zero_freq <- function(construct_level_df) {
  # df mapping zero-frequency structures to their EGP-assigned level
  zero_feats_level <- data.frame(feature = zero_constructs)
  zero_feats_level <- add_feature_level(zero_feats_level)

  cefr_levels <- c("A1", "A2", "B1", "B2", "C1", "C2")

  percent_df <- data.frame(level = cefr_levels, percentage_zero_freq = rep(NA, 6), n_zero_freq = rep(NA, 6))

  for (lvl in cefr_levels) {
    # All structures of the level
    n_constr_level <- construct_level_df %>%
      filter(feat_level == lvl) %>%
      count()
    # Only structures with zero-frequency of the level
    n_zero_feats_lvl <- zero_feats_level %>%
      filter(feat_level == lvl) %>%
      count()
    # calculate the percentage
    percent_df[percent_df$level == lvl,]$percentage_zero_freq <- n_zero_feats_lvl / n_constr_level * 100
    percent_df[percent_df$level == lvl,]$n_zero_freq <- n_zero_feats_lvl
  }

  return(percent_df)
}

get_percent_constr_zero_freq(constr_lvl_df)
```

Now we observe how the structures for which the Kruskal-Wallis test found significant differences are distributed across EGP levels  

```{r}
# Calculate the percentage of structures of each level that were informative (Kruskal-Wallis)
get_percent_informative <- function(construct_level_df = constr_lvl_df, informative_constr = kw_sign_constructs, zero_constr = zero_constructs) {
  cefr_levels <- c("A1", "A2", "B1", "B2", "C1")
  ret_df <- data.frame(EGP_lvl = cefr_levels, num_informative_constr = rep(NA, 5), percent_informative_level = rep(NA, 5))

  #Iterate over CEFR levels
  for (lvl in cefr_levels) {
    #count all structures of the level that are not zero
    num_found_at_level <- construct_level_df %>%
      filter(feat_level == lvl & !(feature %in% zero_constr)) %>%
      count()

    num_significant_at_level <- construct_level_df %>%
      filter(feat_level == lvl & feature %in% informative_constr) %>%
      count()

    ret_df[ret_df$EGP_lvl == lvl, ]$num_informative_constr <- num_significant_at_level
    ret_df[ret_df$EGP_lvl == lvl, ]$percent_informative_level <- num_significant_at_level / num_found_at_level * 100
  }

  return(ret_df)

}

kable(get_percent_informative(), format = "latex")
```

What percentage of structures show a significant difference at their EGP-assigned level?

```{r}

# Number of structures per EGP level that presented difference at the EGP level
num_per_level_sign <- function(predict_df){
  cefr_levels <- c("A1", "A2", "B1", "B2", "C1")
  ret_df <- data.frame(EGP_lvl = cefr_levels, signific_at_EGP_lvl = rep(NA, 5), signific_at_EGP_lvl_total = rep(NA, 5))

  # Only structures that were significant at their EGP-assigned level
  sign_at_level <- predict_df %>%
    filter(signif_at_EGP_level == TRUE)

  for (lvl in cefr_levels){
    n <- sign_at_level %>% # number of structures significant at this level
      filter(EGP_level == lvl) %>%
      count()

    n_all <- predict_df %>% # all structures at this level
      filter(EGP_level == lvl) %>%
      count()

    ret_df[ret_df$EGP_lvl == lvl, ]$signific_at_EGP_lvl <- n / n_all * 100
    ret_df[ret_df$EGP_lvl == lvl, ]$signific_at_EGP_lvl_total <- n
  }

  return(ret_df)
}

num_per_level_sign(wcx_level_predict)

```

In total, what percentage of structures show a significant difference at their EGP-level?

```{r}
accuracy_signif_at_level(wcx_level_predict)
```

Now, we sample two structures from each EGP level to find in the data:
```{r}
set.seed(1234)

# non-zero structures with their EGP level
non_zero_with_lvl <- as.data.frame(constructs)
colnames(non_zero_with_lvl) <- c("feature")
non_zero_with_lvl <- add_feature_level(non_zero_with_lvl)

print(paste("A1", sample(non_zero_with_lvl[non_zero_with_lvl["feat_level"] == "A1", ]$feature, size=2)))
print(paste("A2", sample(non_zero_with_lvl[non_zero_with_lvl["feat_level"] == "A2", ]$feature, size=2)))
print(paste("B1", sample(non_zero_with_lvl[non_zero_with_lvl["feat_level"] == "B1", ]$feature, size=2)))
print(paste("B2", sample(non_zero_with_lvl[non_zero_with_lvl["feat_level"] == "B2", ]$feature, size=2)))
print(paste("C1", sample(non_zero_with_lvl[non_zero_with_lvl["feat_level"] == "C1", ]$feature, size=2)))

```