---
title: "TODO paper title"
output:
  pdf_document: default
  html_notebook: default
---


## Preparing the environment

```{r}
setwd("/home/daniela/Documents/ARAL_EGP_EFCAMDAT/R_files/")
source("/home/daniela/Documents/ARAL_EGP_EFCAMDAT/R_files/functions.R")
library(ggplot2)
library(gridExtra)
library("dplyr")
library("stringr")
library(tidyr)
library(tools)
library(knitr)
library("tidytext")
library("tokenizers")
load("/home/daniela/Documents/ARAL_EGP_EFCAMDAT/R_files/ef2_20181231.RData")
all_features <- readLines("all_features.txt")
opts_chunk$set(cache = TRUE)
```

First, I we get rid of all texts that have not achieved the passing grade of X (TODO)
```{r}
# passing_grade <- 80 #TODO change?

# head(ef2)
# ef2_filtered <- ef2[ef2$grade >= passing_grade, ]

# nrow(ef2)
```

We are left with 832810 texts. Now, we add a column with the CEFR level that each text should belong to based on the EFCAMDAT level

```{r}
# rename the "level" column (which is at index 2)
colnames(ef2)[2] <- "ef_level"
# remove unnecessary colums
ef2 <- ef2[, !colnames(ef2) %in% c("text", "corrected", "nationality")]
# add cefr level
ef2_prepared <- add_cefr_from_ef_levels(ef2)

# mark those rows with the first and last units of a CEFR level
# This will help us group them
ef2_prepared <- add_combined_level(ef2_prepared)
```

Now, get the learnerIDs of people who have finished their respective CEFR levels

```{r}
# Get those students that completed each level
# (except C2, for which we only need the students who completed the first three units)


cefr_ef_lvls <- list(
  "A1" = c(1:3),
  "A2" = c(4:6),
  "B1" = c(7:9),
  "B2" = c(10:12),
  "C1" = c(13:15),
  "C2" = c(16)
)

# A dataframe containing the (ef_level, unit) pairs that we are interested in
ef_lvl_unit_df <- data.frame(ef_lvl = sapply(cefr_ef_lvls, ))
ef_lvl_unit_df
list_students_finsihed <- list()


for (cur_cefr_lvl in names(cefr_ef_lvls)) {
  # Minimum number of texts the learner should have completed in the subset that will be taken into account
  min_n_texts <- 6 # (only top 3 and highest and lowest units in the level)

  if (cur_cefr_lvl == "A1") {
    min_n_texts <- 9 # top 6 lowest + top 3 highest
  } else if (cur_cefr_lvl == "C2") {
    min_n_texts <- 3 # top 3 lowest
  }

  learners_finished <- ef2_prepared %>%
    filter(cefr_level == cur_cefr_lvl & !is.na(combined_cefr)) %>% # current cefr level top high/low units
    group_by(learnerID) %>%
    summarise(units_count = n_distinct(ef_level, unit)) %>% # number of unique ef_level-unit pairs each learner has completed
    filter(units_count == min_n_texts) %>% # only those that have completed all units at least once
    pull(learnerID)

  list_students_finsihed[[cur_cefr_lvl]] <- learners_finished
}

# Check how many students we have for each level:
for (lvl in names(list_students_finsihed)) {
  print(paste(lvl, length(list_students_finsihed[[lvl]])))
}

# 49 is the minimum number of students having finished a level (C1)
min_num_students <- min(sapply(list_students_finsihed, length))
```

We now know that in order to have a balanced sample, we can have at most 49 students of any given level in the combined CEFR levels.
Namely, each combined class will have 49 students from the lower level and 49 students from the higher level.

From the learners who completed the levels, we must sample 49 without replacement.
These people's texts in the first and last three units of their CEFR levels will be the ones to be analyzed.
In total, at each combined level we will have scripts written by 98 students and we will use them as a "snapshot" of their proficiency once the lower level reached.

Now, we must sample the students and extract their texts to be analyzed by the POLKE tool.

```{r}
sampled_learners <- list()
set.seed(123)
for (level in names(cefr_ef_lvls)) {
  sampled_learners[[level]] <- sample(list_students_finsihed[[level]], min_num_students, replace = FALSE)
}

# To check that the sampling is deterministic, run this line several times and check the learnerIDs
# sapply(sampled_learners, function(x) sort(unique(x)))
```

Students have been sampled. Now, we need to extract their texts and put them in .txt files to be analyzed by the python script.

```{r}
input_path <- "/home/daniela/Documents/ARAL_EGP_EFCAMDAT/text_annotation/data/input_15Nov24/"
df_filtered_all <- ef2_prepared[0, ]

# make dataframe with the learners that we want at each level
for (lvl in names(cefr_ef_lvls)) {
  filtered_df <- ef2_prepared %>%
    filter(cefr_level == lvl & learnerID %in% sampled_learners[[lvl]] & !is.na(combined_cefr))

  df_filtered_all <- rbind(df_filtered_all, filtered_df)
}

# Put the texts into .txt files
# ef_text_2_txt(df_filtered_all, input_path)
```

Now, we run the Python script and wait. It will take a while. 

Once the scripts have been annotated, we need to count how many times each student at the individual CEFR levels have used each construct.

```{r}
# For getting the output path dynamically given the level
# file_format can be txt or csv
get_output_path <- function(combined_lvl, file_format = "csv") {
  # NOTE change the output path if needed
  return(paste0("../text_annotation/data/output_15Nov24/", combined_lvl, "/", file_format, "/"))
}

get_learner_ids_by_lvl <- function(combined_lvl) {
  last_lvl <- str_split_i(combined_lvl, "_", 2) # high level in the combination
  first_lvl <- str_split_i(combined_lvl, "_", 1)
  if (last_lvl == "A1") { # If it's A0_A1, all learners are from A1
    learner_ids <- sampled_learners[["A1"]]
  } else { # get learners from the last level and the first one
    learner_ids <- sort(unique(append(sampled_learners[[first_lvl]], sampled_learners[[last_lvl]])))
  }
}

# Calls the get_feat_count_per_student with the necessary parameters given the level combination
count_constrs_by_lvl <- function(combined_lvl, make_long = TRUE) {
  learner_ids <- get_learner_ids_by_lvl(combined_lvl)

  return(get_feat_count_per_student(directory_path = get_output_path(combined_lvl),
    learner_ids = learner_ids,
    make_long = make_long
  ))
}

# learner_wc_by_level <- 

# These dfs contain the feature counts for all learners at each combined level
a0a1_feat_counts <- count_constrs_by_lvl("A0_A1", make_long = FALSE)
a1a2_feat_counts <- count_constrs_by_lvl("A1_A2", make_long = FALSE)
a2b1_feat_counts <- count_constrs_by_lvl("A2_B1", make_long = FALSE)
b1b2_feat_counts <- count_constrs_by_lvl("B1_B2", make_long = FALSE)
b2c1_feat_counts <- count_constrs_by_lvl("B2_C1", make_long = FALSE)
c1c2_feat_counts <- count_constrs_by_lvl("C1_C2", make_long = FALSE)
```

Now it is necessary to normalize these counts by number of words written by each individual student in the respective (combined) levels and normalize it by 100 words

```{r}
a0a1_word_counts <- get_learner_word_count_df(get_learner_ids_by_lvl("A0_A1"), get_output_path("A0_A1", "text"))
a1a2_word_counts <- get_learner_word_count_df(get_learner_ids_by_lvl("A1_A2"), get_output_path("A1_A2", "text"))
a2b1_word_counts <- get_learner_word_count_df(get_learner_ids_by_lvl("A2_B1"), get_output_path("A2_B1", "text"))
b1b2_word_counts <- get_learner_word_count_df(get_learner_ids_by_lvl("B1_B2"), get_output_path("B1_B2", "text"))
b2c1_word_counts <- get_learner_word_count_df(get_learner_ids_by_lvl("B2_C1"), get_output_path("B2_C1", "text"))
c1c2_word_counts <- get_learner_word_count_df(get_learner_ids_by_lvl("C1_C2"), get_output_path("C1_C2", "text"))


a0a1_normalized <- cbind(get_feats_normalized_students(a0a1_feat_counts, a0a1_word_counts), level = "A0_A1")
a1a2_normalized <- cbind(get_feats_normalized_students(a1a2_feat_counts, a1a2_word_counts), level = "A1_A2")
a2b1_normalized <- cbind(get_feats_normalized_students(a2b1_feat_counts, a2b1_word_counts), level = "A2_B1")
b1b2_normalized <- cbind(get_feats_normalized_students(b1b2_feat_counts, b1b2_word_counts), level = "B1_B2")
b2c1_normalized <- cbind(get_feats_normalized_students(b2c1_feat_counts, b2c1_word_counts), level = "B2_C1")
c1c2_normalized <- cbind(get_feats_normalized_students(c1c2_feat_counts, c1c2_word_counts), level = "C1_C2")
```

Now we know the normalized frequency of each costruct for the students at each level combination. 


First, we get rid of all constructs which were not found in any student's writings:


```{r}
# put all dataframes together
normalized_all_levels <- rbind(a0a1_normalized, a1a2_normalized, a2b1_normalized, b1b2_normalized, b2c1_normalized, c1c2_normalized)

# if there are any NAs, it's because texts by some students could not be processed. Drop these rows
normalized_all_levels <- normalized_all_levels %>%
  drop_na()

zero_constructs <- normalized_all_levels %>% 
  group_by(feature) %>%
  filter(all(total == 0)) %>%
  pull(feature) %>%
  unique()

# remove these zero-constructs from our data
normalized_all_levels <- normalized_all_levels %>%
  filter(!feature %in% zero_constructs)
```
We can start statistically analyzing the construct frequency distributions per learner. 

